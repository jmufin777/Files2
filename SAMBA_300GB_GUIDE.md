# üñ•Ô∏è Samba + 300 GB Setup - Pokyn

## ‚úÖ Ano, je to sch≈Ødn√©!

Ale pot≈ôebujete tyto optimalizace pro 300 GB textu s Word/Excel soubory.

---

## üìã **Sch≈Ødnost faktory**

### 1. **Poƒçet soubor≈Ø**
```
300 GB Excel/Word
Pr≈Ømƒõr. soubor: 1-5 MB (office files)
‚Üí Cca 60,000 - 300,000 soubor≈Ø
‚Üí Zvl√°dnete ‚úÖ (mƒõl byste)
```

### 2. **Samba Performance**
```
Samba throughput: 50-150 MB/s (lok√°ln√≠ s√≠≈•)
300 GB data:
  - P≈ôi 50 MB/s = 1.6 hodin (initial scan)
  - P≈ôi 150 MB/s = 30 minut (good network)

Incremental (jen nov√© soubory): Rychlej≈°√≠! ‚úÖ
```

### 3. **Indexov√°n√≠ ƒças**
```
300 GB ‚Üí 300K chunks (1000 char chunks)
Per chunk (embedding): ~200ms
Paralleln√≠ (batch 10): ~20ms
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Celkem: 6,000 minut / 10 = 600 minut = 10 hodin

S caching: 5 hodin ‚úÖ
```

### 4. **Storage na disku**
```
PostgreSQL vectorstore (300K chunks):
  - Vectors: 300K √ó 768 dims √ó 4 bytes = 900 MB
  - Metadata: +200 MB
  - Index: +200 MB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CELKEM: ~1.5 GB 

Zvl√°dnete ‚úÖ (i s 500GB diskem)
```

---

## üîß **Setup instrukt√°≈æ**

### **Krok 1: P≈ôipojit Samba na Linux/macOS**

#### **macOS (Finder)**
```bash
# P≈ôidat s√≠≈•ov√© um√≠stƒõn√≠
# Finder ‚Üí Go ‚Üí Connect to Server
# smb://username@192.168.1.100/documents

# Nebo z CLI:
mkdir -p ~/mnt/documents
mount_smbfs //username:password@192.168.1.100/documents ~/mnt/documents
```

#### **Linux (Ubuntu)**
```bash
sudo apt-get install cifs-utils

# Vytvo≈ôit mount point
mkdir -p /mnt/samba

# P≈ôipojit
sudo mount -t cifs //192.168.1.100/documents /mnt/samba \
  -o username=user,password=pass,uid=1000,gid=1000

# Permanentnƒõ (v /etc/fstab)
//192.168.1.100/documents /mnt/samba cifs credentials=/home/user/.smbcredentials,uid=1000,gid=1000 0 0
```

### **Krok 2: Testovat p≈ôipojen√≠**
```bash
# Ovƒõ≈ôit
ls -la /mnt/samba
du -sh /mnt/samba  # Zobrazit velikost

# Mƒõ≈ôit speed
time cp /mnt/samba/test.xlsx /tmp/test.xlsx
```

### **Krok 3: API endpoint pro Sambu**

Nov√Ω endpoint `/api/samba` (nyn√≠ dostupn√Ω!) skenuje Samba sd√≠len√≠:

```typescript
// POST /api/samba
{
  "sambaPath": "/mnt/samba",
  "recursive": true,
  "maxFiles": 1000  // Limit pro prvn√≠ sken
}
```

**Response:**
```json
{
  "success": true,
  "files": [
    {
      "path": "/mnt/samba/docs/report.xlsx",
      "name": "docs/report.xlsx",
      "size": 2048576,
      "type": "file",
      "modified": "2024-01-15T10:30:00Z"
    }
  ],
  "stats": {
    "totalFiles": 45123,
    "totalSize": 321474836480,
    "totalSizeGB": "299.24"
  }
}
```

### **Krok 4: Extrahovat text z Office**

Nov√Ω endpoint `/api/extract` parsuje Word/Excel:

```typescript
// POST /api/extract
{
  "filePath": "/mnt/samba/docs/report.xlsx",
  "fileName": "report.xlsx"
}
```

**Podporovan√© typy:**
- `docx`, `doc` - Word
- `xlsx`, `xls` - Excel
- `pdf` - PDF
- `txt`, `md`, `csv` - Text

---

## üöÄ **Optimalizaƒçn√≠ strategie pro 300 GB**

### **1. Batch Processing (D≈ÆLE≈ΩIT√â!)**

Neindexujte v≈°echno najednou. Rozdƒõlte na batche:

```bash
# Batch 1: 10 GB
# Batch 2: 10 GB
# ...
# Batch 30: 10 GB

# Nebo:
# Batch per folder
# Batch per department
```

**P≈ô√≠klad workflow:**
```
Pondƒõl√≠:     Indexuj "Finance" (20 GB)
√öter√Ω:       Indexuj "HR" (15 GB)
St≈ôeda:      Indexuj "Operations" (25 GB)
... a tak d√°l
```

### **2. Caching (Performance)**

P≈ôidej Redis cache pro extractovan√© texty:

```bash
npm install redis
```

```typescript
// Cache extracted text pro 1 hodinu
const cacheKey = `extracted:${filePath}:${stat.mtime}`;
const cached = await redis.get(cacheKey);
if (cached) return JSON.parse(cached);

// Extrahuj a cachuj
const text = await extractText(filePath);
await redis.setex(cacheKey, 3600, JSON.stringify(text));
```

### **3. Incremental Sync (KL√çƒåOV√â)**

Monitorujte Sambu na zmƒõny:

```bash
npm install chokidar  # File watcher
```

```typescript
// Sledovat Sambu na nov√©/upraven√© soubory
const watcher = chokidar.watch("/mnt/samba", {
  persistent: true,
  ignored: ["node_modules", ".git"],
  awaitWriteFinish: {
    stabilityThreshold: 2000,
    pollInterval: 100,
  },
});

watcher.on("change", async (path) => {
  console.log(`File changed: ${path}`);
  // Re-index jen tenhle soubor
  await indexSingleFile(path);
});
```

### **4. Parallel Extraction (Rychlost)**

```typescript
// M√≠sto sekvenƒçnƒõ, zpracuj parallelnƒõ
const { pLimit } = await import("p-limit");
const limit = pLimit(5); // 5 paralleln√≠ch

const extractPromises = files.map((file) =>
  limit(() => extractText(file.path))
);

const texts = await Promise.all(extractPromises);
```

### **5. Memory-efficient Chunking**

```typescript
// Nesplituj v≈°e najednou - streamuj
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 800,      // Men≈°√≠ chunks = lep≈°√≠ relevance
  chunkOverlap: 150,
});

// Zpracuj po 100 souborech najednou
for (let i = 0; i < files.length; i += 100) {
  const batch = files.slice(i, i + 100);
  await indexBatch(batch);
  console.log(`Indexed ${i + 100}/${files.length}`);
}
```

---

## üìä **Oƒçek√°van√Ω Timeline**

### **Skenov√°n√≠ Samby (300 GB)**
```
30,000 soubor≈Ø √ó 10ms = 5 minut
Inicializace DB:        5 minut
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CELKEM SKEN:            10 minut
```

### **Extrakce textu**
```
30,000 soubor≈Ø √ó 50ms (extraction) = 25 minut
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Paralelnƒõ (5x):         5 minut
```

### **Indexov√°n√≠ (embeddings)**
```
300K chunks √ó 200ms (sequential) = 60,000 s = 16.6 hodin
Paralelnƒõ (10x batch):  ~1.6 hodin
S caching:              ~4 hodin (first run)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CELKEM (first):         5 hodin
Incremental:            10-30 minut
```

### **Celkovƒõ (first time)**
```
Sken:       10 minut
Extrakce:   5 minut (paralelnƒõ)
Indexing:   4 hodin (s optimalizac√≠)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:      4.25 hodin (p≈ôes noc je ide√°ln√≠!)
```

---

## üíæ **Storage Requirements**

```
Original data:           300 GB
PostgreSQL vectorstore:  ~1.5 GB (vektory)
Cache (Redis):           ~5 GB (full extract)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CELKEM:                  ~310 GB
```

**Disk na serveru:**
- Minim√°ln√≠: 500 GB (tƒõsnƒõ)
- Doporuƒçeno: **1 TB** (prostor pro logs, temp files)

---

## üîê **Bezpeƒçnost Samby**

### Kredenci√°ly
```bash
# Negeneruj v k√≥du! Pou≈æij env vars:
SAMBA_PATH=/mnt/samba
SAMBA_USER=documents_user
SAMBA_PASS=secure_password_here

# Nebo authentication file (Linux):
cat ~/.smbcredentials
username=user
password=pass

chmod 600 ~/.smbcredentials
```

### Firewall
```bash
# Jen lok√°ln√≠ s√≠tƒõ na Sambu
sudo ufw allow from 192.168.1.0/24 to any port 445
sudo ufw allow from 192.168.1.0/24 to any port 139
```

---

## üõ†Ô∏è **Deployment Checklist**

- [ ] Samba p≈ôipojeno a testov√°no (`mount | grep samba`)
- [ ] Ovƒõ≈ôit read permissions na v≈°ech slo≈æk√°ch
- [ ] PostgreSQL bƒõ≈æ√≠ s dost disk space
- [ ] `SAMBA_PATH` v `.env.local`
- [ ] Testovat `/api/samba` endpoint
- [ ] Testovat `/api/extract` s jedn√≠m souborem
- [ ] Spustit indexov√°n√≠ v noci (batch re≈æim)
- [ ] Monitorovat disk space bƒõhem indexov√°n√≠
- [ ] Nastavit watcher pro nov√© soubory

---

## üìà **Performance Monitoring**

P≈ôidej metriky:

```typescript
console.time("samba-scan");
const files = await scanSamba();
console.timeEnd("samba-scan");

console.time("extract-batch");
const texts = await extractBatch(files);
console.timeEnd("extract-batch");

console.time("index-batch");
await indexBatch(texts);
console.timeEnd("index-batch");
```

---

## ‚ö†Ô∏è **Probl√©my & ≈òe≈°en√≠**

| Probl√©m | ≈òe≈°en√≠ |
|---------|---------|
| "Permission denied" na Sambu | Ovƒõ≈ôit permissi: `chmod 755` / `ls -la` |
| Samba disconnects | Nastavit keep-alive v mount options |
| Pomala extrakce Word | Rozdƒõlit na batche, zv√Ω≈°it parallelizaci |
| Out of memory | Sn√≠≈æit `maxFiles` nebo `chunkSize` |
| PostgreSQL disk pln√Ω | Smazat star√© chunks p≈ôed re-indexem |

---

## üöÄ **P≈ô√≠≈°t√≠ kroky**

1. ‚úÖ Setup Samby na va≈°em serveru
2. ‚úÖ Testovat `/api/samba` endpoint
3. ‚úÖ Extrahovat jeden soubor s `/api/extract`
4. ‚úÖ Spustit batch indexov√°n√≠
5. ‚úÖ Nastavit incremental watcher
6. ‚úÖ Optimalizovat chunk size podle relevance

---

## üìû **Reference**

- Samba troubleshooting: https://ubuntu.com/server/docs/samba
- Office parsing: https://github.com/mozilla/pdf.js (PDF)
- Performance tuning: https://wiki.samba.org/index.php/Performance_Tuning
